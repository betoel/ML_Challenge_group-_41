{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0255333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the nltk package is not installed, execute te following:\n",
    "\n",
    "#! pip install stopwords\n",
    "#! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5538b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1a6f2",
   "metadata": {},
   "source": [
    "# 0. Loading vanilla dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29833b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1  c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3  7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4  07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "\n",
       "                                               title   authorId  \\\n",
       "0  Detecting linguistic idiosyncratic interests i...    3188285   \n",
       "1  Bigrams and BiLSTMs Two Neural Networks for Se...    2782720   \n",
       "2  In Factuality: Efficient Integration of Releva...  144748442   \n",
       "3  Variational Graph Autoencoding as Cheap Superv...   46331602   \n",
       "4  LIMIT-BERT : Linguistics Informed Multi-Task BERT   30887404   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "0  Masoud Rouhizadeh  Children with autism spectrum disorder often e...  2014   \n",
       "1       Yuri Bizzoni  We present and compare two alternative deep ne...  2018   \n",
       "2      Peter Vickers  Visual Question Answering (VQA) methods aim at...  2021   \n",
       "3           Irene Li  Coreference resolution over semantic graphs li...  2022   \n",
       "4         Junru Zhou  In this paper, we present Linguistics Informed...  2019   \n",
       "\n",
       "                venue  \n",
       "0         CLPsych@ACL  \n",
       "1  Fig-Lang@NAACL-HLT  \n",
       "2                 ACL  \n",
       "3                 ACL  \n",
       "4            FINDINGS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw = pd.read_json('data/train.json')\n",
    "train_df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71595ded",
   "metadata": {},
   "source": [
    "# A. Frequency lists for abstracts and titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1effc",
   "metadata": {},
   "source": [
    "### A.1 Creating an ordered list of most frequent filtered words in the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34654e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the abstracts\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the abstracts in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'abstract' to 'title' to get the information about the titles\n",
    "X = []\n",
    "for item in data:\n",
    "    abstract = item.get('abstract')\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9 ]\",\"\",abstract)\n",
    "    X.append(abstract.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 1.000 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_abstracts = {k:v for (k,v) in ordered.items() if v > 1000}\n",
    "orderedListAbstracts = []\n",
    "for item in orderedDict_abstracts:\n",
    "    orderedListAbstracts.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d323db",
   "metadata": {},
   "source": [
    "### A.2 Creating an ordered list of most frequent filtered words in the Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fc896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the Titles\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the titles in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'title' to 'abstract' to get the information about the abstracts\n",
    "X = []\n",
    "for item in data:\n",
    "    title = item.get('title')\n",
    "    title = re.sub(\"[^a-zA-Z0-9 ]\",\"\",title)\n",
    "    X.append(title.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 100 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_titles = {k:v for (k,v) in ordered.items() if v > 100}\n",
    "orderedListTitles = []\n",
    "for item in orderedDict_titles:\n",
    "    orderedListTitles.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854e788",
   "metadata": {},
   "source": [
    "### A.3 Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef569d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 140 ['language', 'learning', 'neural', 'translation', 'using', 'task', 'machine', 'models', 'word', 'text'] ['model', 'models', 'language', 'task', 'data', 'paper', 'show', 'results', 'system', 'performance']\n"
     ]
    }
   ],
   "source": [
    "x = len(orderedListTitles)\n",
    "y = len(orderedListAbstracts)\n",
    "\n",
    "print(x, y, orderedListTitles[0:10], orderedListAbstracts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a690b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dictionaries of word frequencies over determined thresholds\n",
    "# orderedDict_titles\n",
    "# orderedDict_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5d6f8",
   "metadata": {},
   "source": [
    "# B. Frequency vectors for titles and abstracts and for each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446170eb",
   "metadata": {},
   "source": [
    "__Important__: Note that the list of keywords for both abstract and titles employed in this section are the same as those defined in the previous one (orderedListTitles and orderedListAbstracts) so modifying those above and then re-running the code below will produce the obtention of different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4be595",
   "metadata": {},
   "source": [
    "Two dictionaries are created to store the number of appearences of a given keyword in titles and abstracts respectively. Subsequently a function is defined to \"reset\" this dictionary given that such action will have to be carried out for each instance of the dataset. (Each dictionary is meant to store ocurrences of abstract and titles of a single instance, the reseted and go on to the next one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b35bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero_titles = np.zeros(len(orderedListTitles))\n",
    "freq_counter_title = {}\n",
    "freq_counter_abstract = {}\n",
    "\n",
    "#frequency dictionary to store appearences of keyowrds (titles)\n",
    "def reset_freq_vectors():\n",
    "    '''\n",
    "    Function to reset the values of the frequency dictionaries that generate the vectors for titles and abstracts\n",
    "    '''\n",
    "    for i in orderedListTitles:\n",
    "        freq_counter_title[i] = 0\n",
    "    for i in orderedListAbstracts:\n",
    "        freq_counter_abstract[i] = 0\n",
    "    #print(\"freq. vectors succesfully reseted\")\n",
    "\n",
    "reset_freq_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bafecf",
   "metadata": {},
   "source": [
    "### B.1 Vectors of absolute frequency of keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31546c7b",
   "metadata": {},
   "source": [
    "Now two new dictionaries are created to store the respective vectors of each instance. Then the loop will look for and then count occurrences of keywords for both abstracts and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da23790b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "Current iteration: 1000\n",
      "Current iteration: 2000\n",
      "Current iteration: 3000\n",
      "Current iteration: 4000\n",
      "Current iteration: 5000\n",
      "Current iteration: 6000\n",
      "Current iteration: 7000\n",
      "Current iteration: 8000\n",
      "Current iteration: 9000\n",
      "Current iteration: 10000\n",
      "Current iteration: 11000\n",
      "Current iteration: 12000\n"
     ]
    }
   ],
   "source": [
    "# These two dictionaries will contain one vector for each instance of the set (title and abstract).\n",
    "all_title_freq_vectors = {}\n",
    "all_abstract_freq_vectors = {}\n",
    "\n",
    "\n",
    "# Looping through the entire dataset\n",
    "for i in range(12129):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Current iteration: {i}\")\n",
    "    \n",
    "    # Comparing titles and titles keywords to check for coincidences and their frequency\n",
    "    title = train_df_raw.iloc[i]['title'].lower()\n",
    "    \n",
    "    for a in orderedListTitles:\n",
    "        for e in train_df_raw.iloc[i]['title'].lower().split():\n",
    "            if a == e:\n",
    "                freq_counter_title[a] += 1\n",
    "                \n",
    "    all_title_freq_vectors[i] = np.array(list(freq_counter_title.values()))\n",
    "    \n",
    "    \n",
    "    # Comparing abstracts and abstracts keywords to check for coincidences and their frequency\n",
    "    abstract = train_df_raw.iloc[i]['abstract'].lower()  \n",
    "    \n",
    "    for a in orderedListAbstracts:\n",
    "        for e in train_df_raw.iloc[i]['abstract'].lower().split():\n",
    "            if a == e:\n",
    "                freq_counter_abstract[a] += 1\n",
    "                \n",
    "    all_abstract_freq_vectors[i] = np.array(list(freq_counter_abstract.values()))\n",
    "    \n",
    "    reset_freq_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d17c2",
   "metadata": {},
   "source": [
    "### B.2 Binary vectors of presence of keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74bbd6",
   "metadata": {},
   "source": [
    "Instead of iterating through the whole dataset again, the frequency vectors for titles and abstract are taken and all non zero values are converted ones while respecting the existing structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa25994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "Current iteration: 1000\n",
      "Current iteration: 2000\n",
      "Current iteration: 3000\n",
      "Current iteration: 4000\n",
      "Current iteration: 5000\n",
      "Current iteration: 6000\n",
      "Current iteration: 7000\n",
      "Current iteration: 8000\n",
      "Current iteration: 9000\n",
      "Current iteration: 10000\n",
      "Current iteration: 11000\n",
      "Current iteration: 12000\n"
     ]
    }
   ],
   "source": [
    "all_title_binary_vectors = {}\n",
    "all_abstract_binary_vectors = {}\n",
    "\n",
    "for i in range(12129):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Current iteration: {i}\")\n",
    "    \n",
    "    transitional_title = []\n",
    "    \n",
    "    element_a = all_title_freq_vectors.get(i)\n",
    "    for e in element_a:\n",
    "        if e != 0:\n",
    "            transitional_title.append(1)\n",
    "        else:\n",
    "            transitional_title.append(0)\n",
    "    all_title_binary_vectors[i] = np.array(transitional_title)\n",
    "    \n",
    "    transitional_abstract = []\n",
    "    \n",
    "    element_a = all_abstract_freq_vectors.get(i)\n",
    "    for e in element_a:\n",
    "        if e != 0:\n",
    "            transitional_abstract.append(1)\n",
    "        else:\n",
    "            transitional_abstract.append(0)\n",
    "    all_abstract_binary_vectors[i] = np.array(transitional_abstract)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fb431",
   "metadata": {},
   "source": [
    "### B.3 Saving (and then loading) the relevant dictionaries as pickle files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d024c7e",
   "metadata": {},
   "source": [
    "Given the relatively high cost of executing the code above, the produced dictionaries containing the vectors are saved as pickle files and then re-loaded (with different names) for subsequent use in the coming sections if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3f400",
   "metadata": {},
   "source": [
    "Code from: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864789ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Saving data\n",
    "###############\n",
    "\n",
    "with open('data/freq_titles.pickle', 'wb') as fp:\n",
    "    pickle.dump(all_title_freq_vectors, fp)\n",
    "    \n",
    "with open('data/freq_abstracts.pickle', 'wb') as fp:\n",
    "    pickle.dump(all_abstract_freq_vectors, fp)\n",
    "    \n",
    "with open('data/binary_titles.pickle', 'wb') as fp:\n",
    "    pickle.dump(all_title_binary_vectors, fp)\n",
    "    \n",
    "with open('data/binary_abstracts.pickle', 'wb') as fp:\n",
    "    pickle.dump(all_abstract_binary_vectors, fp)    \n",
    "\n",
    "    \n",
    "    \n",
    "###############\n",
    "#Loading data\n",
    "###############    \n",
    "        \n",
    "with open('data/freq_titles.pickle', 'rb') as handle:\n",
    "    freq_vector_title = pickle.load(handle)\n",
    "    \n",
    "with open('data/freq_abstracts.pickle', 'rb') as handle:\n",
    "    freq_vector_abstract = pickle.load(handle)\n",
    "    \n",
    "with open('data/binary_titles.pickle', 'rb') as handle:\n",
    "    binary_vector_title = pickle.load(handle)\n",
    "    \n",
    "with open('data/binary_abstracts.pickle', 'rb') as handle:\n",
    "    binary_vector_abstract = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad091f",
   "metadata": {},
   "source": [
    "### B.4 Including frequency vectors in raw df (abtracs absolute frequency in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2cb4f",
   "metadata": {},
   "source": [
    "Creating a dictionary to store the values of the abstarct frequency vectors by keyword so the vectors can be added as one hot econdoed variables to the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e331e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dict = {}\n",
    "for i in range(len(freq_vector_abstract.get(0))):\n",
    "\n",
    "    circumstantial_list = []\n",
    "    for e in freq_vector_abstract.keys():\n",
    "        circumstantial_list.append(freq_vector_abstract.get(e)[i])\n",
    "    one_hot_dict[i] = circumstantial_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780da0a8",
   "metadata": {},
   "source": [
    "Creating a dictionary to store the values of the abstarct binary vectors by keyword so the vectors can be added as one hot econdoed variables to the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a9c3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dict_binary = {}\n",
    "for i in range(len(binary_vector_abstract.get(0))):\n",
    "\n",
    "    circumstantial_list = []\n",
    "    for e in binary_vector_abstract.keys():\n",
    "        circumstantial_list.append(binary_vector_abstract.get(e)[i])\n",
    "    one_hot_dict_binary[i] = circumstantial_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47b1a3",
   "metadata": {},
   "source": [
    "Adding the new features to a copy of the original dataframe and then dropping all other features (Some features deleted in the process for observational simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3556191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391/603293365.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  one_hot_abstract_df[orderedListAbstracts[i]] = one_hot_dict_binary.get(i)\n"
     ]
    }
   ],
   "source": [
    "one_hot_abstract_df = train_df_raw.copy()\n",
    "\n",
    "for i in range(len(freq_vector_abstract.get(0))):\n",
    "    one_hot_abstract_df[orderedListAbstracts[i]] = one_hot_dict_binary.get(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f60fd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>authorId</th>\n",
       "      <th>year</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>language</th>\n",
       "      <th>task</th>\n",
       "      <th>data</th>\n",
       "      <th>paper</th>\n",
       "      <th>show</th>\n",
       "      <th>...</th>\n",
       "      <th>learn</th>\n",
       "      <th>various</th>\n",
       "      <th>experimental</th>\n",
       "      <th>understanding</th>\n",
       "      <th>automatically</th>\n",
       "      <th>often</th>\n",
       "      <th>discourse</th>\n",
       "      <th>prediction</th>\n",
       "      <th>significantly</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>3188285</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>2782720</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>144748442</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>46331602</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>30887404</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12124</th>\n",
       "      <td>5868a7bfe6a4590d332ca66b8097dbe5490c8a73</td>\n",
       "      <td>2001128224</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>6fbfa7138235b99df43391bff3917b85393c3ca1</td>\n",
       "      <td>3209288</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>7226d14e6dea73dfad521256248ec2b19ae66ad8</td>\n",
       "      <td>144254013</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12127</th>\n",
       "      <td>cb0f3ee1e98faf92429d601cdcd76c69c1e484eb</td>\n",
       "      <td>46236380</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12128</th>\n",
       "      <td>30248bea9a739ddba04ec633ee3e156ca8a35e24</td>\n",
       "      <td>144928136</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12129 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paperId    authorId  year  model  \\\n",
       "0      0b341b6938308a6d5f47edf490f6e46eae3835fa     3188285  2014      1   \n",
       "1      c682727ee058aadbe9dbf838dcb036322818f588     2782720  2018      1   \n",
       "2      0f9b5b32229a7245e43754430c0c88f8e7f0d8af   144748442  2021      1   \n",
       "3      7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9    46331602  2022      1   \n",
       "4      07588dd5d0252c7abc99b3834a81bf23741ead4b    30887404  2019      1   \n",
       "...                                         ...         ...   ...    ...   \n",
       "12124  5868a7bfe6a4590d332ca66b8097dbe5490c8a73  2001128224  2020      0   \n",
       "12125  6fbfa7138235b99df43391bff3917b85393c3ca1     3209288  2016      0   \n",
       "12126  7226d14e6dea73dfad521256248ec2b19ae66ad8   144254013  2021      0   \n",
       "12127  cb0f3ee1e98faf92429d601cdcd76c69c1e484eb    46236380  2018      0   \n",
       "12128  30248bea9a739ddba04ec633ee3e156ca8a35e24   144928136  2014      0   \n",
       "\n",
       "       models  language  task  data  paper  show  ...  learn  various  \\\n",
       "0           0         0     0     0      0     0  ...      0        0   \n",
       "1           1         0     0     0      0     0  ...      0        0   \n",
       "2           1         0     0     1      0     0  ...      1        0   \n",
       "3           0         0     0     1      0     1  ...      0        0   \n",
       "4           0         1     1     1      0     0  ...      0        0   \n",
       "...       ...       ...   ...   ...    ...   ...  ...    ...      ...   \n",
       "12124       0         0     0     0      0     1  ...      0        0   \n",
       "12125       0         0     1     0      0     0  ...      0        0   \n",
       "12126       0         0     1     0      0     0  ...      0        0   \n",
       "12127       1         0     0     0      1     0  ...      1        0   \n",
       "12128       0         0     0     0      0     0  ...      0        0   \n",
       "\n",
       "       experimental  understanding  automatically  often  discourse  \\\n",
       "0                 0              0              0      1          0   \n",
       "1                 0              0              0      0          0   \n",
       "2                 0              0              0      0          0   \n",
       "3                 0              0              1      0          0   \n",
       "4                 0              0              0      0          0   \n",
       "...             ...            ...            ...    ...        ...   \n",
       "12124             0              0              0      0          0   \n",
       "12125             0              0              0      0          0   \n",
       "12126             0              0              0      0          0   \n",
       "12127             0              0              0      0          0   \n",
       "12128             0              0              0      0          0   \n",
       "\n",
       "       prediction  significantly  standard  \n",
       "0               0              0         0  \n",
       "1               0              0         0  \n",
       "2               0              0         0  \n",
       "3               0              1         0  \n",
       "4               0              0         0  \n",
       "...           ...            ...       ...  \n",
       "12124           0              0         1  \n",
       "12125           0              0         0  \n",
       "12126           0              0         0  \n",
       "12127           0              0         0  \n",
       "12128           0              0         0  \n",
       "\n",
       "[12129 rows x 143 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_abstract_df.drop(['title', 'abstract', 'venue', 'authorName'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232290f4",
   "metadata": {},
   "source": [
    "### B.5 Employing association rules (apriori algorithm) to find the most common combinations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e36da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_rule_df = one_hot_abstract_df.copy()\n",
    "assoc_rule_df_lean = assoc_rule_df.drop(['paperId','title','authorId','authorName','abstract','year','venue'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f2538a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 200 combinations | Sampling itemset size 4 3\n"
     ]
    }
   ],
   "source": [
    "assoc_show = apriori(assoc_rule_df_lean, min_support = 0.05, use_colnames = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41fd9b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.372001</td>\n",
       "      <td>(model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.304724</td>\n",
       "      <td>(models)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344381</td>\n",
       "      <td>(language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259461</td>\n",
       "      <td>(task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229120</td>\n",
       "      <td>(data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.226152</td>\n",
       "      <td>(paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.350565</td>\n",
       "      <td>(show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.292440</td>\n",
       "      <td>(results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.182373</td>\n",
       "      <td>(system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.225410</td>\n",
       "      <td>(performance)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.277682</td>\n",
       "      <td>(using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.173963</td>\n",
       "      <td>(text)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.197461</td>\n",
       "      <td>(learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.215681</td>\n",
       "      <td>(approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.170088</td>\n",
       "      <td>(word)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.182785</td>\n",
       "      <td>(information)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.124660</td>\n",
       "      <td>(translation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.279331</td>\n",
       "      <td>(propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.115178</td>\n",
       "      <td>(tasks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.186165</td>\n",
       "      <td>(training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.235963</td>\n",
       "      <td>(two)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.214280</td>\n",
       "      <td>(neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.170830</td>\n",
       "      <td>(method)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.202078</td>\n",
       "      <td>(different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.223349</td>\n",
       "      <td>(based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.136120</td>\n",
       "      <td>(work)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.126226</td>\n",
       "      <td>(systems)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.146261</td>\n",
       "      <td>(methods)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.216176</td>\n",
       "      <td>(also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.191689</td>\n",
       "      <td>(new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.141726</td>\n",
       "      <td>(semantic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.218979</td>\n",
       "      <td>(present)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.083189</td>\n",
       "      <td>(languages)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.111139</td>\n",
       "      <td>(knowledge)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.188804</td>\n",
       "      <td>(use)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.114436</td>\n",
       "      <td>(dataset)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.104131</td>\n",
       "      <td>(features)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.169511</td>\n",
       "      <td>(used)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.100503</td>\n",
       "      <td>(datasets)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.122516</td>\n",
       "      <td>(evaluation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.093165</td>\n",
       "      <td>(words)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.168769</td>\n",
       "      <td>(experiments)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.146343</td>\n",
       "      <td>(machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.152774</td>\n",
       "      <td>(natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.098194</td>\n",
       "      <td>(sentence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.092753</td>\n",
       "      <td>(representations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.095226</td>\n",
       "      <td>(corpus)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.119960</td>\n",
       "      <td>(analysis)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.142056</td>\n",
       "      <td>(proposed)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.127381</td>\n",
       "      <td>(first)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.079396</td>\n",
       "      <td>(generation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.114601</td>\n",
       "      <td>(human)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.143293</td>\n",
       "      <td>(novel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.126226</td>\n",
       "      <td>(one)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.088136</td>\n",
       "      <td>(english)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.106604</td>\n",
       "      <td>(set)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.090857</td>\n",
       "      <td>(problem)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.065051</td>\n",
       "      <td>(embeddings)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.129112</td>\n",
       "      <td>(existing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.091928</td>\n",
       "      <td>(approaches)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.082529</td>\n",
       "      <td>(classification)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.073708</td>\n",
       "      <td>(sentences)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.063732</td>\n",
       "      <td>(parsing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.119631</td>\n",
       "      <td>(large)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.118064</td>\n",
       "      <td>(well)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.083024</td>\n",
       "      <td>(framework)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.068266</td>\n",
       "      <td>(context)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.078819</td>\n",
       "      <td>(nlp)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.087229</td>\n",
       "      <td>(research)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.113942</td>\n",
       "      <td>(previous)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.124577</td>\n",
       "      <td>(demonstrate)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.068678</td>\n",
       "      <td>(question)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.079232</td>\n",
       "      <td>(accuracy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.109407</td>\n",
       "      <td>(better)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.079974</td>\n",
       "      <td>(representation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.081952</td>\n",
       "      <td>(network)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.058702</td>\n",
       "      <td>(domain)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.084673</td>\n",
       "      <td>(linguistic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.098194</td>\n",
       "      <td>(trained)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.075027</td>\n",
       "      <td>(target)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.107923</td>\n",
       "      <td>(several)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.071234</td>\n",
       "      <td>(input)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.101245</td>\n",
       "      <td>(three)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.087889</td>\n",
       "      <td>(study)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.082035</td>\n",
       "      <td>(available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.096463</td>\n",
       "      <td>(given)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.091599</td>\n",
       "      <td>(multiple)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.106522</td>\n",
       "      <td>(introduce)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.067442</td>\n",
       "      <td>(quality)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.099843</td>\n",
       "      <td>(improve)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.083519</td>\n",
       "      <td>(shared)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.090115</td>\n",
       "      <td>(across)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.094897</td>\n",
       "      <td>(many)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.058620</td>\n",
       "      <td>(attention)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.090774</td>\n",
       "      <td>(best)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.100998</td>\n",
       "      <td>(outperforms)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.066700</td>\n",
       "      <td>(syntactic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.054662</td>\n",
       "      <td>(entity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.095639</td>\n",
       "      <td>(recent)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.068761</td>\n",
       "      <td>(processing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.061341</td>\n",
       "      <td>(structure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.082529</td>\n",
       "      <td>(automatic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.088878</td>\n",
       "      <td>(find)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.064144</td>\n",
       "      <td>(lexical)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.067854</td>\n",
       "      <td>(source)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.078325</td>\n",
       "      <td>(test)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.054168</td>\n",
       "      <td>(extraction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.079726</td>\n",
       "      <td>(simple)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.088301</td>\n",
       "      <td>(evaluate)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.051447</td>\n",
       "      <td>(dependency)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.085085</td>\n",
       "      <td>(provide)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(multilingual)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.065051</td>\n",
       "      <td>(baseline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.080798</td>\n",
       "      <td>(without)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.050622</td>\n",
       "      <td>(algorithm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.083189</td>\n",
       "      <td>(achieves)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.073790</td>\n",
       "      <td>(learn)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.078242</td>\n",
       "      <td>(various)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.084178</td>\n",
       "      <td>(experimental)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.059939</td>\n",
       "      <td>(understanding)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.072224</td>\n",
       "      <td>(automatically)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.079479</td>\n",
       "      <td>(often)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.076593</td>\n",
       "      <td>(significantly)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.070657</td>\n",
       "      <td>(standard)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.155742</td>\n",
       "      <td>(models, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.139500</td>\n",
       "      <td>(model, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.099019</td>\n",
       "      <td>(task, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.090692</td>\n",
       "      <td>(model, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.065545</td>\n",
       "      <td>(model, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.154258</td>\n",
       "      <td>(show, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.121527</td>\n",
       "      <td>(model, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.050045</td>\n",
       "      <td>(model, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.107264</td>\n",
       "      <td>(performance, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.109737</td>\n",
       "      <td>(using, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.069833</td>\n",
       "      <td>(text, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.087064</td>\n",
       "      <td>(model, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.084426</td>\n",
       "      <td>(approach, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.066040</td>\n",
       "      <td>(word, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073790</td>\n",
       "      <td>(information, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.143046</td>\n",
       "      <td>(propose, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.051200</td>\n",
       "      <td>(tasks, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.089620</td>\n",
       "      <td>(model, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.093660</td>\n",
       "      <td>(two, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.114766</td>\n",
       "      <td>(neural, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.067936</td>\n",
       "      <td>(method, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.080468</td>\n",
       "      <td>(different, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.088878</td>\n",
       "      <td>(model, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.052354</td>\n",
       "      <td>(work, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.056476</td>\n",
       "      <td>(methods, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.083271</td>\n",
       "      <td>(model, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.077253</td>\n",
       "      <td>(model, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.052436</td>\n",
       "      <td>(model, semantic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.077006</td>\n",
       "      <td>(present, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.073625</td>\n",
       "      <td>(use, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.060186</td>\n",
       "      <td>(model, used)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.079974</td>\n",
       "      <td>(experiments, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.053508</td>\n",
       "      <td>(model, machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.053591</td>\n",
       "      <td>(model, natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.074037</td>\n",
       "      <td>(proposed, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.065958</td>\n",
       "      <td>(novel, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.055734</td>\n",
       "      <td>(model, existing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.056146</td>\n",
       "      <td>(model, previous)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.060186</td>\n",
       "      <td>(demonstrate, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.053261</td>\n",
       "      <td>(better, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.064391</td>\n",
       "      <td>(model, outperforms)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.138429</td>\n",
       "      <td>(models, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.079232</td>\n",
       "      <td>(models, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.081293</td>\n",
       "      <td>(models, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.130761</td>\n",
       "      <td>(models, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.100338</td>\n",
       "      <td>(models, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.094319</td>\n",
       "      <td>(performance, models)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.089785</td>\n",
       "      <td>(models, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.057301</td>\n",
       "      <td>(models, text)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.072553</td>\n",
       "      <td>(models, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.058867</td>\n",
       "      <td>(models, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.054497</td>\n",
       "      <td>(models, word)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.055899</td>\n",
       "      <td>(models, information)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.102234</td>\n",
       "      <td>(models, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.052107</td>\n",
       "      <td>(models, tasks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.080468</td>\n",
       "      <td>(models, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.078737</td>\n",
       "      <td>(models, two)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.093083</td>\n",
       "      <td>(models, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.069668</td>\n",
       "      <td>(models, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.066865</td>\n",
       "      <td>(models, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.071976</td>\n",
       "      <td>(models, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.065463</td>\n",
       "      <td>(models, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.060104</td>\n",
       "      <td>(present, models)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.059279</td>\n",
       "      <td>(use, models)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.065298</td>\n",
       "      <td>(models, experiments)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.054250</td>\n",
       "      <td>(models, natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.091269</td>\n",
       "      <td>(task, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.091681</td>\n",
       "      <td>(language, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.071069</td>\n",
       "      <td>(paper, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.120208</td>\n",
       "      <td>(show, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.099101</td>\n",
       "      <td>(language, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.059032</td>\n",
       "      <td>(language, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.080716</td>\n",
       "      <td>(performance, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.099101</td>\n",
       "      <td>(using, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.063402</td>\n",
       "      <td>(text, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.072801</td>\n",
       "      <td>(language, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.070575</td>\n",
       "      <td>(approach, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.058867</td>\n",
       "      <td>(word, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.057548</td>\n",
       "      <td>(information, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.052107</td>\n",
       "      <td>(translation, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.095226</td>\n",
       "      <td>(propose, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.054250</td>\n",
       "      <td>(tasks, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.072388</td>\n",
       "      <td>(training, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.078325</td>\n",
       "      <td>(two, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.073295</td>\n",
       "      <td>(neural, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.059774</td>\n",
       "      <td>(method, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.070904</td>\n",
       "      <td>(different, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.073873</td>\n",
       "      <td>(language, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.051117</td>\n",
       "      <td>(work, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.051694</td>\n",
       "      <td>(methods, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.082200</td>\n",
       "      <td>(language, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.067936</td>\n",
       "      <td>(new, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.075521</td>\n",
       "      <td>(present, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.073625</td>\n",
       "      <td>(use, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.062412</td>\n",
       "      <td>(used, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.059527</td>\n",
       "      <td>(experiments, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.059609</td>\n",
       "      <td>(language, machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.135955</td>\n",
       "      <td>(language, natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.055487</td>\n",
       "      <td>(language, processing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.066123</td>\n",
       "      <td>(task, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.072801</td>\n",
       "      <td>(task, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.081375</td>\n",
       "      <td>(task, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.084920</td>\n",
       "      <td>(task, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.069256</td>\n",
       "      <td>(task, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.061753</td>\n",
       "      <td>(performance, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.076841</td>\n",
       "      <td>(using, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(text, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.058537</td>\n",
       "      <td>(task, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.058950</td>\n",
       "      <td>(approach, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.069915</td>\n",
       "      <td>(propose, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.052354</td>\n",
       "      <td>(task, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.068513</td>\n",
       "      <td>(two, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.055322</td>\n",
       "      <td>(neural, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.057548</td>\n",
       "      <td>(task, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.063567</td>\n",
       "      <td>(task, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.055899</td>\n",
       "      <td>(task, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.050622</td>\n",
       "      <td>(task, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.054085</td>\n",
       "      <td>(present, task)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.056806</td>\n",
       "      <td>(task, shared)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.050375</td>\n",
       "      <td>(paper, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.086157</td>\n",
       "      <td>(show, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.073625</td>\n",
       "      <td>(data, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.064309</td>\n",
       "      <td>(performance, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.074615</td>\n",
       "      <td>(using, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.058208</td>\n",
       "      <td>(learning, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.051282</td>\n",
       "      <td>(approach, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.066123</td>\n",
       "      <td>(propose, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.084261</td>\n",
       "      <td>(training, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.057383</td>\n",
       "      <td>(two, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.053838</td>\n",
       "      <td>(neural, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.051859</td>\n",
       "      <td>(different, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.052107</td>\n",
       "      <td>(data, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.053426</td>\n",
       "      <td>(also, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.051859</td>\n",
       "      <td>(present, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.050458</td>\n",
       "      <td>(use, data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.059032</td>\n",
       "      <td>(show, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.062660</td>\n",
       "      <td>(paper, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.061835</td>\n",
       "      <td>(paper, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.063154</td>\n",
       "      <td>(using, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.050045</td>\n",
       "      <td>(approach, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.051447</td>\n",
       "      <td>(two, paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.056311</td>\n",
       "      <td>(paper, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.140737</td>\n",
       "      <td>(show, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.098277</td>\n",
       "      <td>(performance, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.107099</td>\n",
       "      <td>(using, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.064886</td>\n",
       "      <td>(text, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.081540</td>\n",
       "      <td>(show, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.082200</td>\n",
       "      <td>(approach, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.065793</td>\n",
       "      <td>(word, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.070245</td>\n",
       "      <td>(information, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.126639</td>\n",
       "      <td>(propose, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.077500</td>\n",
       "      <td>(show, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.087476</td>\n",
       "      <td>(two, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.089290</td>\n",
       "      <td>(neural, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.074037</td>\n",
       "      <td>(method, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.076593</td>\n",
       "      <td>(show, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.077748</td>\n",
       "      <td>(show, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.050870</td>\n",
       "      <td>(work, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.059939</td>\n",
       "      <td>(methods, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.086817</td>\n",
       "      <td>(show, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.073460</td>\n",
       "      <td>(show, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.069750</td>\n",
       "      <td>(present, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.069750</td>\n",
       "      <td>(use, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.060351</td>\n",
       "      <td>(show, used)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.096545</td>\n",
       "      <td>(experiments, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.052436</td>\n",
       "      <td>(show, machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.052436</td>\n",
       "      <td>(show, natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.061176</td>\n",
       "      <td>(proposed, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.055899</td>\n",
       "      <td>(novel, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.053755</td>\n",
       "      <td>(show, existing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.050952</td>\n",
       "      <td>(show, previous)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.051612</td>\n",
       "      <td>(better, show)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.055075</td>\n",
       "      <td>(show, outperforms)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.074285</td>\n",
       "      <td>(performance, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.088301</td>\n",
       "      <td>(using, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.050952</td>\n",
       "      <td>(text, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.066287</td>\n",
       "      <td>(learning, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.066947</td>\n",
       "      <td>(approach, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.053343</td>\n",
       "      <td>(word, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.057878</td>\n",
       "      <td>(information, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.095968</td>\n",
       "      <td>(propose, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.057053</td>\n",
       "      <td>(training, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.083107</td>\n",
       "      <td>(two, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.072801</td>\n",
       "      <td>(neural, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.062412</td>\n",
       "      <td>(method, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.065628</td>\n",
       "      <td>(different, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.070740</td>\n",
       "      <td>(based, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.051035</td>\n",
       "      <td>(methods, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.067689</td>\n",
       "      <td>(also, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.058455</td>\n",
       "      <td>(new, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.061011</td>\n",
       "      <td>(present, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.057218</td>\n",
       "      <td>(use, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.055982</td>\n",
       "      <td>(proposed, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.073213</td>\n",
       "      <td>(results, experimental)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.052354</td>\n",
       "      <td>(using, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.070080</td>\n",
       "      <td>(performance, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.055240</td>\n",
       "      <td>(performance, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.053096</td>\n",
       "      <td>(performance, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.076593</td>\n",
       "      <td>(performance, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.062248</td>\n",
       "      <td>(performance, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.059939</td>\n",
       "      <td>(performance, two)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.061670</td>\n",
       "      <td>(performance, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.050622</td>\n",
       "      <td>(performance, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.052024</td>\n",
       "      <td>(performance, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.054333</td>\n",
       "      <td>(performance, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.051035</td>\n",
       "      <td>(using, text)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.061011</td>\n",
       "      <td>(using, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.070327</td>\n",
       "      <td>(approach, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.054168</td>\n",
       "      <td>(using, word)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.053013</td>\n",
       "      <td>(using, information)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.076841</td>\n",
       "      <td>(propose, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.058785</td>\n",
       "      <td>(using, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.067607</td>\n",
       "      <td>(two, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.066535</td>\n",
       "      <td>(neural, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.055157</td>\n",
       "      <td>(method, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.058455</td>\n",
       "      <td>(using, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.061918</td>\n",
       "      <td>(using, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.064721</td>\n",
       "      <td>(using, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.056641</td>\n",
       "      <td>(using, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.062660</td>\n",
       "      <td>(present, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.058372</td>\n",
       "      <td>(use, using)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.051117</td>\n",
       "      <td>(using, used)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.052354</td>\n",
       "      <td>(propose, text)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.052107</td>\n",
       "      <td>(approach, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.067029</td>\n",
       "      <td>(propose, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.051942</td>\n",
       "      <td>(training, learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.074697</td>\n",
       "      <td>(propose, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.053013</td>\n",
       "      <td>(two, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.052519</td>\n",
       "      <td>(neural, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.061588</td>\n",
       "      <td>(approach, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.052849</td>\n",
       "      <td>(present, approach)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.062660</td>\n",
       "      <td>(propose, information)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.089620</td>\n",
       "      <td>(translation, machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.064391</td>\n",
       "      <td>(propose, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.083849</td>\n",
       "      <td>(propose, two)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.081870</td>\n",
       "      <td>(propose, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.073048</td>\n",
       "      <td>(method, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.059279</td>\n",
       "      <td>(propose, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.074285</td>\n",
       "      <td>(propose, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.056311</td>\n",
       "      <td>(propose, methods)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.061588</td>\n",
       "      <td>(propose, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.064309</td>\n",
       "      <td>(propose, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.052107</td>\n",
       "      <td>(use, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.071234</td>\n",
       "      <td>(propose, experiments)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.066287</td>\n",
       "      <td>(propose, proposed)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.064968</td>\n",
       "      <td>(propose, novel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.055982</td>\n",
       "      <td>(propose, existing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(propose, demonstrate)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.051035</td>\n",
       "      <td>(propose, outperforms)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.054497</td>\n",
       "      <td>(neural, training)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.055322</td>\n",
       "      <td>(two, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.060021</td>\n",
       "      <td>(two, different)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.057383</td>\n",
       "      <td>(two, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.051859</td>\n",
       "      <td>(two, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.053096</td>\n",
       "      <td>(neural, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.055899</td>\n",
       "      <td>(neural, machine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.058950</td>\n",
       "      <td>(neural, network)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(different, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.054333</td>\n",
       "      <td>(present, based)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.051612</td>\n",
       "      <td>(new, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.055075</td>\n",
       "      <td>(present, also)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.070822</td>\n",
       "      <td>(models, model, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.070822</td>\n",
       "      <td>(models, show, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.052684</td>\n",
       "      <td>(models, model, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.053426</td>\n",
       "      <td>(performance, models, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.060021</td>\n",
       "      <td>(models, model, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.052684</td>\n",
       "      <td>(models, model, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.057795</td>\n",
       "      <td>(show, model, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(propose, model, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.065298</td>\n",
       "      <td>(show, model, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.066947</td>\n",
       "      <td>(propose, show, model)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.051447</td>\n",
       "      <td>(propose, model, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.051859</td>\n",
       "      <td>(propose, model, neural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.059362</td>\n",
       "      <td>(models, show, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>(models, language, natural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.053013</td>\n",
       "      <td>(models, show, results)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.051200</td>\n",
       "      <td>(models, show, propose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.054168</td>\n",
       "      <td>(propose, show, results)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                      itemsets\n",
       "0    0.372001                       (model)\n",
       "1    0.304724                      (models)\n",
       "2    0.344381                    (language)\n",
       "3    0.259461                        (task)\n",
       "4    0.229120                        (data)\n",
       "5    0.226152                       (paper)\n",
       "6    0.350565                        (show)\n",
       "7    0.292440                     (results)\n",
       "8    0.182373                      (system)\n",
       "9    0.225410                 (performance)\n",
       "10   0.277682                       (using)\n",
       "11   0.173963                        (text)\n",
       "12   0.197461                    (learning)\n",
       "13   0.215681                    (approach)\n",
       "14   0.170088                        (word)\n",
       "15   0.182785                 (information)\n",
       "16   0.124660                 (translation)\n",
       "17   0.279331                     (propose)\n",
       "18   0.115178                       (tasks)\n",
       "19   0.186165                    (training)\n",
       "20   0.235963                         (two)\n",
       "21   0.214280                      (neural)\n",
       "22   0.170830                      (method)\n",
       "23   0.202078                   (different)\n",
       "24   0.223349                       (based)\n",
       "25   0.136120                        (work)\n",
       "26   0.126226                     (systems)\n",
       "27   0.146261                     (methods)\n",
       "28   0.216176                        (also)\n",
       "29   0.191689                         (new)\n",
       "30   0.141726                    (semantic)\n",
       "31   0.218979                     (present)\n",
       "32   0.083189                   (languages)\n",
       "33   0.111139                   (knowledge)\n",
       "34   0.188804                         (use)\n",
       "35   0.114436                     (dataset)\n",
       "36   0.104131                    (features)\n",
       "37   0.169511                        (used)\n",
       "38   0.100503                    (datasets)\n",
       "39   0.122516                  (evaluation)\n",
       "40   0.093165                       (words)\n",
       "41   0.168769                 (experiments)\n",
       "42   0.146343                     (machine)\n",
       "43   0.152774                     (natural)\n",
       "44   0.098194                    (sentence)\n",
       "45   0.092753             (representations)\n",
       "46   0.095226                      (corpus)\n",
       "47   0.119960                    (analysis)\n",
       "48   0.142056                    (proposed)\n",
       "49   0.127381                       (first)\n",
       "50   0.079396                  (generation)\n",
       "51   0.114601                       (human)\n",
       "52   0.143293                       (novel)\n",
       "53   0.126226                         (one)\n",
       "54   0.088136                     (english)\n",
       "55   0.106604                         (set)\n",
       "56   0.090857                     (problem)\n",
       "57   0.065051                  (embeddings)\n",
       "58   0.129112                    (existing)\n",
       "59   0.091928                  (approaches)\n",
       "60   0.082529              (classification)\n",
       "61   0.073708                   (sentences)\n",
       "62   0.063732                     (parsing)\n",
       "63   0.119631                       (large)\n",
       "64   0.118064                        (well)\n",
       "65   0.083024                   (framework)\n",
       "66   0.068266                     (context)\n",
       "67   0.078819                         (nlp)\n",
       "68   0.087229                    (research)\n",
       "69   0.113942                    (previous)\n",
       "70   0.124577                 (demonstrate)\n",
       "71   0.068678                    (question)\n",
       "72   0.079232                    (accuracy)\n",
       "73   0.109407                      (better)\n",
       "74   0.079974              (representation)\n",
       "75   0.081952                     (network)\n",
       "76   0.058702                      (domain)\n",
       "77   0.084673                  (linguistic)\n",
       "78   0.098194                     (trained)\n",
       "79   0.075027                      (target)\n",
       "80   0.107923                     (several)\n",
       "81   0.071234                       (input)\n",
       "82   0.101245                       (three)\n",
       "83   0.087889                       (study)\n",
       "84   0.082035                   (available)\n",
       "85   0.096463                       (given)\n",
       "86   0.091599                    (multiple)\n",
       "87   0.106522                   (introduce)\n",
       "88   0.067442                     (quality)\n",
       "89   0.099843                     (improve)\n",
       "90   0.083519                      (shared)\n",
       "91   0.090115                      (across)\n",
       "92   0.094897                        (many)\n",
       "93   0.058620                   (attention)\n",
       "94   0.090774                        (best)\n",
       "95   0.100998                 (outperforms)\n",
       "96   0.066700                   (syntactic)\n",
       "97   0.054662                      (entity)\n",
       "98   0.095639                      (recent)\n",
       "99   0.068761                  (processing)\n",
       "100  0.061341                   (structure)\n",
       "101  0.082529                   (automatic)\n",
       "102  0.088878                        (find)\n",
       "103  0.064144                     (lexical)\n",
       "104  0.067854                      (source)\n",
       "105  0.078325                        (test)\n",
       "106  0.054168                  (extraction)\n",
       "107  0.079726                      (simple)\n",
       "108  0.088301                    (evaluate)\n",
       "109  0.051447                  (dependency)\n",
       "110  0.085085                     (provide)\n",
       "111  0.050128                (multilingual)\n",
       "112  0.065051                    (baseline)\n",
       "113  0.080798                     (without)\n",
       "114  0.050622                   (algorithm)\n",
       "115  0.083189                    (achieves)\n",
       "116  0.073790                       (learn)\n",
       "117  0.078242                     (various)\n",
       "118  0.084178                (experimental)\n",
       "119  0.059939               (understanding)\n",
       "120  0.072224               (automatically)\n",
       "121  0.079479                       (often)\n",
       "122  0.076593               (significantly)\n",
       "123  0.070657                    (standard)\n",
       "124  0.155742               (models, model)\n",
       "125  0.139500             (model, language)\n",
       "126  0.099019                 (task, model)\n",
       "127  0.090692                 (model, data)\n",
       "128  0.065545                (model, paper)\n",
       "129  0.154258                 (show, model)\n",
       "130  0.121527              (model, results)\n",
       "131  0.050045               (model, system)\n",
       "132  0.107264          (performance, model)\n",
       "133  0.109737                (using, model)\n",
       "134  0.069833                 (text, model)\n",
       "135  0.087064             (model, learning)\n",
       "136  0.084426             (approach, model)\n",
       "137  0.066040                 (word, model)\n",
       "138  0.073790          (information, model)\n",
       "139  0.143046              (propose, model)\n",
       "140  0.051200                (tasks, model)\n",
       "141  0.089620             (model, training)\n",
       "142  0.093660                  (two, model)\n",
       "143  0.114766               (neural, model)\n",
       "144  0.067936               (method, model)\n",
       "145  0.080468            (different, model)\n",
       "146  0.088878                (model, based)\n",
       "147  0.052354                 (work, model)\n",
       "148  0.056476              (methods, model)\n",
       "149  0.083271                 (model, also)\n",
       "150  0.077253                  (model, new)\n",
       "151  0.052436             (model, semantic)\n",
       "152  0.077006              (present, model)\n",
       "153  0.073625                  (use, model)\n",
       "154  0.060186                 (model, used)\n",
       "155  0.079974          (experiments, model)\n",
       "156  0.053508              (model, machine)\n",
       "157  0.053591              (model, natural)\n",
       "158  0.074037             (proposed, model)\n",
       "159  0.065958                (novel, model)\n",
       "160  0.055734             (model, existing)\n",
       "161  0.056146             (model, previous)\n",
       "162  0.060186          (demonstrate, model)\n",
       "163  0.053261               (better, model)\n",
       "164  0.064391          (model, outperforms)\n",
       "165  0.138429            (models, language)\n",
       "166  0.079232                (models, task)\n",
       "167  0.081293                (models, data)\n",
       "168  0.130761                (models, show)\n",
       "169  0.100338             (models, results)\n",
       "170  0.094319         (performance, models)\n",
       "171  0.089785               (models, using)\n",
       "172  0.057301                (models, text)\n",
       "173  0.072553            (models, learning)\n",
       "174  0.058867            (models, approach)\n",
       "175  0.054497                (models, word)\n",
       "176  0.055899         (models, information)\n",
       "177  0.102234             (models, propose)\n",
       "178  0.052107               (models, tasks)\n",
       "179  0.080468            (models, training)\n",
       "180  0.078737                 (models, two)\n",
       "181  0.093083              (models, neural)\n",
       "182  0.069668           (models, different)\n",
       "183  0.066865               (models, based)\n",
       "184  0.071976                (models, also)\n",
       "185  0.065463                 (models, new)\n",
       "186  0.060104             (present, models)\n",
       "187  0.059279                 (use, models)\n",
       "188  0.065298         (models, experiments)\n",
       "189  0.054250             (models, natural)\n",
       "190  0.091269              (task, language)\n",
       "191  0.091681              (language, data)\n",
       "192  0.071069             (paper, language)\n",
       "193  0.120208              (show, language)\n",
       "194  0.099101           (language, results)\n",
       "195  0.059032            (language, system)\n",
       "196  0.080716       (performance, language)\n",
       "197  0.099101             (using, language)\n",
       "198  0.063402              (text, language)\n",
       "199  0.072801          (language, learning)\n",
       "200  0.070575          (approach, language)\n",
       "201  0.058867              (word, language)\n",
       "202  0.057548       (information, language)\n",
       "203  0.052107       (translation, language)\n",
       "204  0.095226           (propose, language)\n",
       "205  0.054250             (tasks, language)\n",
       "206  0.072388          (training, language)\n",
       "207  0.078325               (two, language)\n",
       "208  0.073295            (neural, language)\n",
       "209  0.059774            (method, language)\n",
       "210  0.070904         (different, language)\n",
       "211  0.073873             (language, based)\n",
       "212  0.051117              (work, language)\n",
       "213  0.051694           (methods, language)\n",
       "214  0.082200              (language, also)\n",
       "215  0.067936               (new, language)\n",
       "216  0.075521           (present, language)\n",
       "217  0.073625               (use, language)\n",
       "218  0.062412              (used, language)\n",
       "219  0.059527       (experiments, language)\n",
       "220  0.059609           (language, machine)\n",
       "221  0.135955           (language, natural)\n",
       "222  0.055487        (language, processing)\n",
       "223  0.066123                  (task, data)\n",
       "224  0.072801                 (task, paper)\n",
       "225  0.081375                  (task, show)\n",
       "226  0.084920               (task, results)\n",
       "227  0.069256                (task, system)\n",
       "228  0.061753           (performance, task)\n",
       "229  0.076841                 (using, task)\n",
       "230  0.050128                  (text, task)\n",
       "231  0.058537              (task, learning)\n",
       "232  0.058950              (approach, task)\n",
       "233  0.069915               (propose, task)\n",
       "234  0.052354              (task, training)\n",
       "235  0.068513                   (two, task)\n",
       "236  0.055322                (neural, task)\n",
       "237  0.057548             (task, different)\n",
       "238  0.063567                 (task, based)\n",
       "239  0.055899                  (task, also)\n",
       "240  0.050622                   (task, new)\n",
       "241  0.054085               (present, task)\n",
       "242  0.056806                (task, shared)\n",
       "243  0.050375                 (paper, data)\n",
       "244  0.086157                  (show, data)\n",
       "245  0.073625               (data, results)\n",
       "246  0.064309           (performance, data)\n",
       "247  0.074615                 (using, data)\n",
       "248  0.058208              (learning, data)\n",
       "249  0.051282              (approach, data)\n",
       "250  0.066123               (propose, data)\n",
       "251  0.084261              (training, data)\n",
       "252  0.057383                   (two, data)\n",
       "253  0.053838                (neural, data)\n",
       "254  0.051859             (different, data)\n",
       "255  0.052107                 (data, based)\n",
       "256  0.053426                  (also, data)\n",
       "257  0.051859               (present, data)\n",
       "258  0.050458                   (use, data)\n",
       "259  0.059032                 (show, paper)\n",
       "260  0.062660              (paper, results)\n",
       "261  0.061835               (paper, system)\n",
       "262  0.063154                (using, paper)\n",
       "263  0.050045             (approach, paper)\n",
       "264  0.051447                  (two, paper)\n",
       "265  0.056311                (paper, based)\n",
       "266  0.140737               (show, results)\n",
       "267  0.098277           (performance, show)\n",
       "268  0.107099                 (using, show)\n",
       "269  0.064886                  (text, show)\n",
       "270  0.081540              (show, learning)\n",
       "271  0.082200              (approach, show)\n",
       "272  0.065793                  (word, show)\n",
       "273  0.070245           (information, show)\n",
       "274  0.126639               (propose, show)\n",
       "275  0.077500              (show, training)\n",
       "276  0.087476                   (two, show)\n",
       "277  0.089290                (neural, show)\n",
       "278  0.074037                (method, show)\n",
       "279  0.076593             (show, different)\n",
       "280  0.077748                 (show, based)\n",
       "281  0.050870                  (work, show)\n",
       "282  0.059939               (methods, show)\n",
       "283  0.086817                  (show, also)\n",
       "284  0.073460                   (show, new)\n",
       "285  0.069750               (present, show)\n",
       "286  0.069750                   (use, show)\n",
       "287  0.060351                  (show, used)\n",
       "288  0.096545           (experiments, show)\n",
       "289  0.052436               (show, machine)\n",
       "290  0.052436               (show, natural)\n",
       "291  0.061176              (proposed, show)\n",
       "292  0.055899                 (novel, show)\n",
       "293  0.053755              (show, existing)\n",
       "294  0.050952              (show, previous)\n",
       "295  0.051612                (better, show)\n",
       "296  0.055075           (show, outperforms)\n",
       "297  0.074285        (performance, results)\n",
       "298  0.088301              (using, results)\n",
       "299  0.050952               (text, results)\n",
       "300  0.066287           (learning, results)\n",
       "301  0.066947           (approach, results)\n",
       "302  0.053343               (word, results)\n",
       "303  0.057878        (information, results)\n",
       "304  0.095968            (propose, results)\n",
       "305  0.057053           (training, results)\n",
       "306  0.083107                (two, results)\n",
       "307  0.072801             (neural, results)\n",
       "308  0.062412             (method, results)\n",
       "309  0.065628          (different, results)\n",
       "310  0.070740              (based, results)\n",
       "311  0.051035            (methods, results)\n",
       "312  0.067689               (also, results)\n",
       "313  0.058455                (new, results)\n",
       "314  0.061011            (present, results)\n",
       "315  0.057218                (use, results)\n",
       "316  0.055982           (proposed, results)\n",
       "317  0.073213       (results, experimental)\n",
       "318  0.052354               (using, system)\n",
       "319  0.070080          (performance, using)\n",
       "320  0.055240       (performance, learning)\n",
       "321  0.053096       (performance, approach)\n",
       "322  0.076593        (performance, propose)\n",
       "323  0.062248       (performance, training)\n",
       "324  0.059939            (performance, two)\n",
       "325  0.061670         (performance, neural)\n",
       "326  0.050622      (performance, different)\n",
       "327  0.052024          (performance, based)\n",
       "328  0.054333           (performance, also)\n",
       "329  0.051035                 (using, text)\n",
       "330  0.061011             (using, learning)\n",
       "331  0.070327             (approach, using)\n",
       "332  0.054168                 (using, word)\n",
       "333  0.053013          (using, information)\n",
       "334  0.076841              (propose, using)\n",
       "335  0.058785             (using, training)\n",
       "336  0.067607                  (two, using)\n",
       "337  0.066535               (neural, using)\n",
       "338  0.055157               (method, using)\n",
       "339  0.058455            (using, different)\n",
       "340  0.061918                (using, based)\n",
       "341  0.064721                 (using, also)\n",
       "342  0.056641                  (using, new)\n",
       "343  0.062660              (present, using)\n",
       "344  0.058372                  (use, using)\n",
       "345  0.051117                 (using, used)\n",
       "346  0.052354               (propose, text)\n",
       "347  0.052107          (approach, learning)\n",
       "348  0.067029           (propose, learning)\n",
       "349  0.051942          (training, learning)\n",
       "350  0.074697           (propose, approach)\n",
       "351  0.053013               (two, approach)\n",
       "352  0.052519            (neural, approach)\n",
       "353  0.061588             (approach, based)\n",
       "354  0.052849           (present, approach)\n",
       "355  0.062660        (propose, information)\n",
       "356  0.089620        (translation, machine)\n",
       "357  0.064391           (propose, training)\n",
       "358  0.083849                (propose, two)\n",
       "359  0.081870             (propose, neural)\n",
       "360  0.073048             (method, propose)\n",
       "361  0.059279          (propose, different)\n",
       "362  0.074285              (propose, based)\n",
       "363  0.056311            (propose, methods)\n",
       "364  0.061588               (propose, also)\n",
       "365  0.064309                (propose, new)\n",
       "366  0.052107                (use, propose)\n",
       "367  0.071234        (propose, experiments)\n",
       "368  0.066287           (propose, proposed)\n",
       "369  0.064968              (propose, novel)\n",
       "370  0.055982           (propose, existing)\n",
       "371  0.050128        (propose, demonstrate)\n",
       "372  0.051035        (propose, outperforms)\n",
       "373  0.054497            (neural, training)\n",
       "374  0.055322                 (two, neural)\n",
       "375  0.060021              (two, different)\n",
       "376  0.057383                  (two, based)\n",
       "377  0.051859                   (two, also)\n",
       "378  0.053096               (neural, based)\n",
       "379  0.055899             (neural, machine)\n",
       "380  0.058950             (neural, network)\n",
       "381  0.050128             (different, also)\n",
       "382  0.054333              (present, based)\n",
       "383  0.051612                   (new, also)\n",
       "384  0.055075               (present, also)\n",
       "385  0.070822     (models, model, language)\n",
       "386  0.070822         (models, show, model)\n",
       "387  0.052684      (models, model, results)\n",
       "388  0.053426  (performance, models, model)\n",
       "389  0.060021      (models, model, propose)\n",
       "390  0.052684       (models, model, neural)\n",
       "391  0.057795       (show, model, language)\n",
       "392  0.050128    (propose, model, language)\n",
       "393  0.065298        (show, model, results)\n",
       "394  0.066947        (propose, show, model)\n",
       "395  0.051447     (propose, model, results)\n",
       "396  0.051859      (propose, model, neural)\n",
       "397  0.059362      (models, show, language)\n",
       "398  0.050128   (models, language, natural)\n",
       "399  0.053013       (models, show, results)\n",
       "400  0.051200       (models, show, propose)\n",
       "401  0.054168      (propose, show, results)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', assoc_show.shape[0]+1)\n",
    "\n",
    "\n",
    "assoc_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00969b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>language</th>\n",
       "      <th>task</th>\n",
       "      <th>data</th>\n",
       "      <th>paper</th>\n",
       "      <th>show</th>\n",
       "      <th>results</th>\n",
       "      <th>system</th>\n",
       "      <th>performance</th>\n",
       "      <th>...</th>\n",
       "      <th>learn</th>\n",
       "      <th>various</th>\n",
       "      <th>experimental</th>\n",
       "      <th>understanding</th>\n",
       "      <th>automatically</th>\n",
       "      <th>often</th>\n",
       "      <th>discourse</th>\n",
       "      <th>prediction</th>\n",
       "      <th>significantly</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12127</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12129 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  models  language  task  data  paper  show  results  system  \\\n",
       "0          1       0         0     0     0      0     0        0       0   \n",
       "1          1       1         0     0     0      0     0        0       0   \n",
       "2          1       1         0     0     1      0     0        0       0   \n",
       "3          1       0         0     0     1      0     1        0       0   \n",
       "4          1       0         1     1     1      0     0        0       0   \n",
       "...      ...     ...       ...   ...   ...    ...   ...      ...     ...   \n",
       "12124      0       0         0     0     0      0     1        0       0   \n",
       "12125      0       0         0     1     0      0     0        1       1   \n",
       "12126      0       0         0     1     0      0     0        1       0   \n",
       "12127      0       1         0     0     0      1     0        0       0   \n",
       "12128      0       0         0     0     0      0     0        1       0   \n",
       "\n",
       "       performance  ...  learn  various  experimental  understanding  \\\n",
       "0                0  ...      0        0             0              0   \n",
       "1                0  ...      0        0             0              0   \n",
       "2                0  ...      1        0             0              0   \n",
       "3                1  ...      0        0             0              0   \n",
       "4                1  ...      0        0             0              0   \n",
       "...            ...  ...    ...      ...           ...            ...   \n",
       "12124            0  ...      0        0             0              0   \n",
       "12125            0  ...      0        0             0              0   \n",
       "12126            0  ...      0        0             0              0   \n",
       "12127            0  ...      1        0             0              0   \n",
       "12128            0  ...      0        0             0              0   \n",
       "\n",
       "       automatically  often  discourse  prediction  significantly  standard  \n",
       "0                  0      1          0           0              0         0  \n",
       "1                  0      0          0           0              0         0  \n",
       "2                  0      0          0           0              0         0  \n",
       "3                  1      0          0           0              1         0  \n",
       "4                  0      0          0           0              0         0  \n",
       "...              ...    ...        ...         ...            ...       ...  \n",
       "12124              0      0          0           0              0         1  \n",
       "12125              0      0          0           0              0         0  \n",
       "12126              0      0          0           0              0         0  \n",
       "12127              0      0          0           0              0         0  \n",
       "12128              0      0          0           0              0         0  \n",
       "\n",
       "[12129 rows x 140 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assoc_rule_df_lean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14b50e",
   "metadata": {},
   "source": [
    "### B.extra  Obtaining the mean vector for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14254996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding frequency vectors to train dataset\n",
    "\n",
    "train_df_raw[\"key_freq_abs\"] = freq_vector_abstract.values()\n",
    "train_df_raw[\"key_freq_title\"] = freq_vector_title.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b11d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABSTRACTS\n",
    "\n",
    "#dictionary -> author : average_vector_abstract\n",
    "author_vector_abs = {}\n",
    "\n",
    "for i in train_df_raw['authorName'].unique():\n",
    "    \n",
    "    end_vec = np.zeros(len(freq_vector_abstract.get(0)))\n",
    "    array_list = []\n",
    "    \n",
    "    for e in (train_df_raw[train_df_raw[\"authorName\"] == i].index):\n",
    "        \n",
    "        array_list.append(freq_vector_abstract[e]) \n",
    "    author_vector_abs[i] = sum(array_list)/len(array_list)\n",
    "        \n",
    "#TITLES\n",
    "\n",
    "#dictionary -> author : average_vector_title\n",
    "author_vector_title = {}\n",
    "\n",
    "for i in train_df_raw['authorName'].unique():\n",
    "    \n",
    "    end_vec = np.zeros(len(freq_vector_title.get(0)))\n",
    "    array_list = []\n",
    "    \n",
    "    for e in (train_df_raw[train_df_raw[\"authorName\"] == i].index):\n",
    "        \n",
    "        \n",
    "        array_list.append(freq_vector_title[e]) \n",
    "    author_vector_title[i] = sum(array_list)/len(array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebb89893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries with the \"mean keywords vectors\" for both abstracts and tiles of each author\n",
    "\n",
    "#author_vector_abs \n",
    "#author_vector_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03555823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 3.66666667, 0.33333333, 0.        , 0.66666667,\n",
       "       0.33333333, 0.        , 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.66666667, 0.        , 0.33333333, 0.        ,\n",
       "       0.33333333, 0.        , 0.66666667, 0.33333333, 0.33333333,\n",
       "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "       0.33333333, 1.66666667, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.66666667, 0.        , 0.        , 0.        ,\n",
       "       0.66666667, 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.33333333, 0.33333333, 0.66666667, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "       0.        , 0.66666667, 0.33333333, 0.        , 3.        ,\n",
       "       0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "       0.33333333, 0.33333333, 0.33333333, 0.        , 1.        ,\n",
       "       1.        , 0.        , 0.33333333, 0.        , 0.33333333,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.66666667, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.        , 0.        , 0.66666667, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_vector_abs.get(\"Alex Warstadt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c750b745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>key_freq_abs</th>\n",
       "      <th>key_freq_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>055fac05cd424e7b1bdcd359ff7980ca8d938ef3</td>\n",
       "      <td>Learning Which Features Matter: RoBERTa Acquir...</td>\n",
       "      <td>46236380</td>\n",
       "      <td>Alex Warstadt</td>\n",
       "      <td>One reason pretraining on self-supervised ling...</td>\n",
       "      <td>2020</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>[1, 5, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>3cd331c997e90f737810aad6fcce4d993315189f</td>\n",
       "      <td>Investigating BERT’s Knowledge of Language: Fi...</td>\n",
       "      <td>46236380</td>\n",
       "      <td>Alex Warstadt</td>\n",
       "      <td>Though state-of-the-art sentence representatio...</td>\n",
       "      <td>2019</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12127</th>\n",
       "      <td>cb0f3ee1e98faf92429d601cdcd76c69c1e484eb</td>\n",
       "      <td>Neural Network Acceptability Judgments</td>\n",
       "      <td>46236380</td>\n",
       "      <td>Alex Warstadt</td>\n",
       "      <td>Abstract This paper investigates the ability o...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Transactions of the Association for Computatio...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paperId  \\\n",
       "7486   055fac05cd424e7b1bdcd359ff7980ca8d938ef3   \n",
       "10196  3cd331c997e90f737810aad6fcce4d993315189f   \n",
       "12127  cb0f3ee1e98faf92429d601cdcd76c69c1e484eb   \n",
       "\n",
       "                                                   title  authorId  \\\n",
       "7486   Learning Which Features Matter: RoBERTa Acquir...  46236380   \n",
       "10196  Investigating BERT’s Knowledge of Language: Fi...  46236380   \n",
       "12127             Neural Network Acceptability Judgments  46236380   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "7486   Alex Warstadt  One reason pretraining on self-supervised ling...  2020   \n",
       "10196  Alex Warstadt  Though state-of-the-art sentence representatio...  2019   \n",
       "12127  Alex Warstadt  Abstract This paper investigates the ability o...  2018   \n",
       "\n",
       "                                                   venue  \\\n",
       "7486                                               EMNLP   \n",
       "10196                                              EMNLP   \n",
       "12127  Transactions of the Association for Computatio...   \n",
       "\n",
       "                                            key_freq_abs  \\\n",
       "7486   [1, 5, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "10196  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12127  [0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          key_freq_title  \n",
       "7486   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10196  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12127  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw[train_df_raw[\"authorName\"] == \"Alex Warstadt\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
