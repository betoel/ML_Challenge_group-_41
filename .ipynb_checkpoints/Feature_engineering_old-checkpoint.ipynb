{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0255333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the nltk package is not installed, execute te following:\n",
    "\n",
    "#! pip install stopwords\n",
    "#! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5538b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1a6f2",
   "metadata": {},
   "source": [
    "# 0. Loading vanilla dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29833b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1  c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3  7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4  07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "\n",
       "                                               title   authorId  \\\n",
       "0  Detecting linguistic idiosyncratic interests i...    3188285   \n",
       "1  Bigrams and BiLSTMs Two Neural Networks for Se...    2782720   \n",
       "2  In Factuality: Efficient Integration of Releva...  144748442   \n",
       "3  Variational Graph Autoencoding as Cheap Superv...   46331602   \n",
       "4  LIMIT-BERT : Linguistics Informed Multi-Task BERT   30887404   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "0  Masoud Rouhizadeh  Children with autism spectrum disorder often e...  2014   \n",
       "1       Yuri Bizzoni  We present and compare two alternative deep ne...  2018   \n",
       "2      Peter Vickers  Visual Question Answering (VQA) methods aim at...  2021   \n",
       "3           Irene Li  Coreference resolution over semantic graphs li...  2022   \n",
       "4         Junru Zhou  In this paper, we present Linguistics Informed...  2019   \n",
       "\n",
       "                venue  \n",
       "0         CLPsych@ACL  \n",
       "1  Fig-Lang@NAACL-HLT  \n",
       "2                 ACL  \n",
       "3                 ACL  \n",
       "4            FINDINGS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw = pd.read_json('data/train.json')\n",
    "train_df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71595ded",
   "metadata": {},
   "source": [
    "# A. Frequency lists for abstracts and titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1effc",
   "metadata": {},
   "source": [
    "### A.1 Creating an ordered list of most frequent filtered words in the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34654e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the abstracts\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the abstracts in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'abstract' to 'title' to get the information about the titles\n",
    "X = []\n",
    "for item in data:\n",
    "    abstract = item.get('abstract')\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9 ]\",\"\",abstract)\n",
    "    X.append(abstract.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 1.000 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_abstracts = {k:v for (k,v) in ordered.items() if v > 1000}\n",
    "orderedListAbstracts = []\n",
    "for item in orderedDict_abstracts:\n",
    "    orderedListAbstracts.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d323db",
   "metadata": {},
   "source": [
    "### A.2 Creating an ordered list of most frequent filtered words in the Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fc896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the Titles\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the titles in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'title' to 'abstract' to get the information about the abstracts\n",
    "X = []\n",
    "for item in data:\n",
    "    title = item.get('title')\n",
    "    title = re.sub(\"[^a-zA-Z0-9 ]\",\"\",title)\n",
    "    X.append(title.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 100 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_titles = {k:v for (k,v) in ordered.items() if v > 100}\n",
    "orderedListTitles = []\n",
    "for item in orderedDict_titles:\n",
    "    orderedListTitles.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854e788",
   "metadata": {},
   "source": [
    "### A.3 Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef569d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 140 ['language', 'learning', 'neural', 'translation', 'using', 'task', 'machine', 'models', 'word', 'text'] ['model', 'models', 'language', 'task', 'data', 'paper', 'show', 'results', 'system', 'performance']\n"
     ]
    }
   ],
   "source": [
    "x = len(orderedListTitles)\n",
    "y = len(orderedListAbstracts)\n",
    "\n",
    "print(x, y, orderedListTitles[0:10], orderedListAbstracts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a690b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dictionaries of word frequencies over determined thresholds\n",
    "# orderedDict_titles\n",
    "# orderedDict_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14dad3",
   "metadata": {},
   "source": [
    "### A.Z Conecting keywords to authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2456d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12129, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce91b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionaries to store the values of the new fetures containing the keywords present in abtracts and titles\n",
    "dict_assoc_title = {}\n",
    "dict_assoc_abstract = {}\n",
    "\n",
    "# Looping through the entire dataset\n",
    "for i in range(12129):\n",
    "    \n",
    "    # Empty temporary list to store keyword coincidences\n",
    "    titlelist_temp = []\n",
    "    abstractlist_temp = []\n",
    "    \n",
    "    # Extracting titles and abstract by index to compare\n",
    "    title = train_df_raw.iloc[i]['title'].lower()\n",
    "    abstract = train_df_raw.iloc[i]['abstract'].lower()\n",
    "    \n",
    "    # Comparing titles and titles keywords to check for coincidences, and adding them to temporary list\n",
    "    for a in orderedListTitles:\n",
    "        if a in title:\n",
    "            titlelist_temp.append(a)\n",
    "    \n",
    "    # Comparing abstracts and abtracts keywords to check for coincidences, and adding them to temporary list\n",
    "    for b in orderedListAbstracts:\n",
    "        if b in abstract:\n",
    "            abstractlist_temp.append(b)\n",
    "            \n",
    "    dict_assoc_title[i] = titlelist_temp\n",
    "    dict_assoc_abstract[i] = abstractlist_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3a256",
   "metadata": {},
   "source": [
    "__Below__: Example of the results of the code above. Displaying an instance's title, and the value of the dictionary storing keyword coincidences for that same instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31fd0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: detecting linguistic idiosyncratic interests in autism using distributional semantic models\n",
      "Keyword coincidences found: ['using', 'models', 'semantic', 'model', 'linguistic', 'detecting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Title: \" + train_df_raw.iloc[0]['title'].lower())\n",
    "print(\"Keyword coincidences found: \" + str(dict_assoc_title[0]))\n",
    "\n",
    "# Lowering/eliminating thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59306aa4",
   "metadata": {},
   "source": [
    "(Provisional) Adding new features to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be06f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_raw['title_keyw'] = dict_assoc_title.values()\n",
    "train_df_raw['abstract_keyw'] = dict_assoc_abstract.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96ed3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>title_keyw</th>\n",
       "      <th>abstract_keyw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "      <td>[using, models, semantic, model, linguistic, d...</td>\n",
       "      <td>[model, models, paper, using, word, also, sema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "      <td>[neural, detection, networks, network]</td>\n",
       "      <td>[model, models, performance, text, word, two, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "      <td>[question, answering, efficient]</td>\n",
       "      <td>[model, models, language, data, paper, results...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "      <td>[graph, resolution, coreference]</td>\n",
       "      <td>[model, task, data, show, performance, using, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "      <td>[task, linguistic, bert]</td>\n",
       "      <td>[model, language, task, data, paper, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12124</th>\n",
       "      <td>5868a7bfe6a4590d332ca66b8097dbe5490c8a73</td>\n",
       "      <td>SmBoP: Semi-autoregressive Bottom-up Semantic ...</td>\n",
       "      <td>2001128224</td>\n",
       "      <td>Ohad Rubin</td>\n",
       "      <td>The de-facto standard decoding method for sema...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>[semantic, parsing]</td>\n",
       "      <td>[model, show, using, approach, propose, traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>6fbfa7138235b99df43391bff3917b85393c3ca1</td>\n",
       "      <td>UW-Stanford System Description for AESW 2016 S...</td>\n",
       "      <td>3209288</td>\n",
       "      <td>D. Flickinger</td>\n",
       "      <td>This is a report on the methods used and resul...</td>\n",
       "      <td>2016</td>\n",
       "      <td>BEA@NAACL-HLT</td>\n",
       "      <td>[task, detection, system, shared]</td>\n",
       "      <td>[task, results, system, method, based, methods...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>7226d14e6dea73dfad521256248ec2b19ae66ad8</td>\n",
       "      <td>From Raw Text to Enhanced Universal Dependenci...</td>\n",
       "      <td>144254013</td>\n",
       "      <td>G. Bouma</td>\n",
       "      <td>We describe the second IWPT task on end-to-end...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IWPT</td>\n",
       "      <td>[task, text, parsing, shared]</td>\n",
       "      <td>[task, data, results, text, approach, training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12127</th>\n",
       "      <td>cb0f3ee1e98faf92429d601cdcd76c69c1e484eb</td>\n",
       "      <td>Neural Network Acceptability Judgments</td>\n",
       "      <td>46236380</td>\n",
       "      <td>Alex Warstadt</td>\n",
       "      <td>Abstract This paper investigates the ability o...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Transactions of the Association for Computatio...</td>\n",
       "      <td>[neural, network]</td>\n",
       "      <td>[model, models, paper, system, two, neural, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12128</th>\n",
       "      <td>30248bea9a739ddba04ec633ee3e156ca8a35e24</td>\n",
       "      <td>Bridging Text and Knowledge with Frames</td>\n",
       "      <td>144928136</td>\n",
       "      <td>S. Narayanan</td>\n",
       "      <td>FrameNet is the best currently operational ver...</td>\n",
       "      <td>2014</td>\n",
       "      <td></td>\n",
       "      <td>[text, knowledge]</td>\n",
       "      <td>[results, system, text, work, systems, semanti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12129 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paperId  \\\n",
       "0      0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1      c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2      0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3      7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4      07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "...                                         ...   \n",
       "12124  5868a7bfe6a4590d332ca66b8097dbe5490c8a73   \n",
       "12125  6fbfa7138235b99df43391bff3917b85393c3ca1   \n",
       "12126  7226d14e6dea73dfad521256248ec2b19ae66ad8   \n",
       "12127  cb0f3ee1e98faf92429d601cdcd76c69c1e484eb   \n",
       "12128  30248bea9a739ddba04ec633ee3e156ca8a35e24   \n",
       "\n",
       "                                                   title    authorId  \\\n",
       "0      Detecting linguistic idiosyncratic interests i...     3188285   \n",
       "1      Bigrams and BiLSTMs Two Neural Networks for Se...     2782720   \n",
       "2      In Factuality: Efficient Integration of Releva...   144748442   \n",
       "3      Variational Graph Autoencoding as Cheap Superv...    46331602   \n",
       "4      LIMIT-BERT : Linguistics Informed Multi-Task BERT    30887404   \n",
       "...                                                  ...         ...   \n",
       "12124  SmBoP: Semi-autoregressive Bottom-up Semantic ...  2001128224   \n",
       "12125  UW-Stanford System Description for AESW 2016 S...     3209288   \n",
       "12126  From Raw Text to Enhanced Universal Dependenci...   144254013   \n",
       "12127             Neural Network Acceptability Judgments    46236380   \n",
       "12128            Bridging Text and Knowledge with Frames   144928136   \n",
       "\n",
       "              authorName                                           abstract  \\\n",
       "0      Masoud Rouhizadeh  Children with autism spectrum disorder often e...   \n",
       "1           Yuri Bizzoni  We present and compare two alternative deep ne...   \n",
       "2          Peter Vickers  Visual Question Answering (VQA) methods aim at...   \n",
       "3               Irene Li  Coreference resolution over semantic graphs li...   \n",
       "4             Junru Zhou  In this paper, we present Linguistics Informed...   \n",
       "...                  ...                                                ...   \n",
       "12124         Ohad Rubin  The de-facto standard decoding method for sema...   \n",
       "12125      D. Flickinger  This is a report on the methods used and resul...   \n",
       "12126           G. Bouma  We describe the second IWPT task on end-to-end...   \n",
       "12127      Alex Warstadt  Abstract This paper investigates the ability o...   \n",
       "12128       S. Narayanan  FrameNet is the best currently operational ver...   \n",
       "\n",
       "       year                                              venue  \\\n",
       "0      2014                                        CLPsych@ACL   \n",
       "1      2018                                 Fig-Lang@NAACL-HLT   \n",
       "2      2021                                                ACL   \n",
       "3      2022                                                ACL   \n",
       "4      2019                                           FINDINGS   \n",
       "...     ...                                                ...   \n",
       "12124  2020                                              NAACL   \n",
       "12125  2016                                      BEA@NAACL-HLT   \n",
       "12126  2021                                               IWPT   \n",
       "12127  2018  Transactions of the Association for Computatio...   \n",
       "12128  2014                                                      \n",
       "\n",
       "                                              title_keyw  \\\n",
       "0      [using, models, semantic, model, linguistic, d...   \n",
       "1                 [neural, detection, networks, network]   \n",
       "2                       [question, answering, efficient]   \n",
       "3                       [graph, resolution, coreference]   \n",
       "4                               [task, linguistic, bert]   \n",
       "...                                                  ...   \n",
       "12124                                [semantic, parsing]   \n",
       "12125                  [task, detection, system, shared]   \n",
       "12126                      [task, text, parsing, shared]   \n",
       "12127                                  [neural, network]   \n",
       "12128                                  [text, knowledge]   \n",
       "\n",
       "                                           abstract_keyw  \n",
       "0      [model, models, paper, using, word, also, sema...  \n",
       "1      [model, models, performance, text, word, two, ...  \n",
       "2      [model, models, language, data, paper, results...  \n",
       "3      [model, task, data, show, performance, using, ...  \n",
       "4      [model, language, task, data, paper, performan...  \n",
       "...                                                  ...  \n",
       "12124  [model, show, using, approach, propose, traini...  \n",
       "12125  [task, results, system, method, based, methods...  \n",
       "12126  [task, data, results, text, approach, training...  \n",
       "12127  [model, models, paper, system, two, neural, wo...  \n",
       "12128  [results, system, text, work, systems, semanti...  \n",
       "\n",
       "[12129 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
